{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "930c0d45",
   "metadata": {},
   "source": [
    "# Data Preparation for Test-Time Scaling\n",
    "\n",
    "This notebook covers:\n",
    "1. Data loading\n",
    "2. Initial exploration\n",
    "3. Data cleaning\n",
    "4. Train/test splitting\n",
    "5. Initial scaling for baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9644087",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from utils.preprocessing import ScalingManager, load_and_preprocess_data, handle_missing_values\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a87c6a0",
   "metadata": {},
   "source": [
    "## 1. Data Loading\n",
    "\n",
    "Load your dataset and perform initial preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ffe7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example with a sample dataset (replace with your actual data)\n",
    "# X, y = load_and_preprocess_data('path_to_your_data.csv', target_column='target')\n",
    "\n",
    "# For demonstration, let's create some synthetic data\n",
    "from sklearn.datasets import make_classification\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, random_state=42)\n",
    "X = pd.DataFrame(X, columns=[f'feature_{i}' for i in range(X.shape[1])])\n",
    "y = pd.Series(y, name='target')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f26b28",
   "metadata": {},
   "source": [
    "## 2. Initial Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169d384b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic statistics\n",
    "print(\"Dataset shape:\", X.shape)\n",
    "print(\"\\nFeature statistics:\")\n",
    "print(X.describe())\n",
    "\n",
    "# Distribution plots\n",
    "plt.figure(figsize=(15, 5))\n",
    "for i, col in enumerate(X.columns[:3]):\n",
    "    plt.subplot(1, 3, i+1)\n",
    "    sns.histplot(X[col], kde=True)\n",
    "    plt.title(f'Distribution of {col}')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6073ba0f",
   "metadata": {},
   "source": [
    "## 3. Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc87f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle missing values if any\n",
    "X = handle_missing_values(X, strategy='mean')\n",
    "\n",
    "# Check for and remove any duplicates\n",
    "X = X.drop_duplicates()\n",
    "y = y[X.index]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82271cc0",
   "metadata": {},
   "source": [
    "## 4. Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0478d764",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(\"Training set shape:\", X_train.shape)\n",
    "print(\"Test set shape:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ebd89f",
   "metadata": {},
   "source": [
    "## 5. Initial Scaling (Baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22d18d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize scaling manager\n",
    "scaling_manager = ScalingManager()\n",
    "\n",
    "# Fit and transform with standard scaler (baseline)\n",
    "scaling_manager.fit_scaler(X_train, 'standard')\n",
    "X_train_scaled = scaling_manager.transform(X_train, 'standard')\n",
    "X_test_scaled = scaling_manager.transform(X_test, 'standard')\n",
    "\n",
    "# Save processed data\n",
    "np.save('../data/processed/X_train.npy', X_train_scaled)\n",
    "np.save('../data/processed/X_test.npy', X_test_scaled)\n",
    "np.save('../data/processed/y_train.npy', y_train)\n",
    "np.save('../data/processed/y_test.npy', y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a46170",
   "metadata": {},
   "source": [
    "## 6. Visualization of Scaled Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6608de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare original vs scaled distributions for a few features\n",
    "plt.figure(figsize=(15, 5))\n",
    "for i, col in enumerate(X_train.columns[:3]):\n",
    "    plt.subplot(1, 3, i+1)\n",
    "    sns.histplot(X_train[col], label='Original', alpha=0.5)\n",
    "    sns.histplot(X_train_scaled[:, i], label='Scaled', alpha=0.5)\n",
    "    plt.title(f'Distribution of {col}')\n",
    "    plt.legend()\n",
    "plt.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
